{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794243c0",
   "metadata": {},
   "source": [
    "# Text Analytics Coursework -- Data Loader\n",
    "\n",
    "This notebook is to help get you started with the datasets used in the coursework assignment. \n",
    "\n",
    "For this coursework, we recommend that you use your virtual environment that you created for the labs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the Emotion dataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e90e82-bbdb-4532-91b1-58229d4f1043",
   "metadata": {},
   "source": [
    "# Social Media Emotion Classification\n",
    "\n",
    "The dataset classifies Tweets into anger, joy, optimism or sadness.\n",
    "\n",
    "First we need to load the data. The data is already split into train, validation and test. The _validation_ set (also called 'development' set or 'devset') can be used to compute performance of your model when tuning hyperparameters, optimising combinations of features, or looking at the errors your model makes before improving it. This allows you to hold out the test set (i.e., not to look at it at all when developing your method) to give a fair evaluation of the model and how well it generalises to new examples. This avoids tuning the model to specific examples in the test set. An alternative approach to validation is to not use a single fixed validation set, but instead use [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf1096-acce-4226-a172-5357f49e91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"train\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "\n",
    "val_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"validation\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Development/validation dataset with {len(val_dataset)} instances loaded\")\n",
    "\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"test\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "\n",
    "# Access the input text and target labels like this...\n",
    "train_texts = train_dataset['text']\n",
    "train_labels = train_dataset['label']\n",
    "\n",
    "val_texts = val_dataset['text']\n",
    "val_labels = val_dataset['label']\n",
    "\n",
    "test_texts = test_dataset['text']\n",
    "test_labels = test_dataset['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af0fbc-2c18-4b68-839d-802595dac600",
   "metadata": {},
   "source": [
    "# Bio Creative V\n",
    "\n",
    "Marks chemicals and diseases in Pubmed articles as named entities. For further details, see [the HuggingFace page](https://huggingface.co/datasets/tner/bc5cdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0376c-93d1-4ee9-a7ba-76bba2afc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset = load_dataset(\n",
    "    \"tner/bc5cdr\", \n",
    ")\n",
    "\n",
    "print(f'The dataset is a dictionary with {len(dataset)} splits: \\n\\n{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515ff00-f8bc-42ac-801f-50224ea50e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It  may be useful to obtain the data in a list format for some sequence tagging methods\n",
    "train_sentences_ner = [item['tokens'] for item in ner_dataset['train']]\n",
    "train_labels_ner = [[str(tag) for tag in item['tags']] for item in ner_dataset['train']]\n",
    "\n",
    "val_sentences_ner = [item['tokens'] for item in ner_dataset['validation']]\n",
    "val_labels_ner = [[str(tag) for tag in item['tags']] for item in ner_dataset['validation']]\n",
    "\n",
    "test_sentences_ner = [item['tokens'] for item in ner_dataset['test']]\n",
    "test_labels_ner = [[str(tag) for tag in item['tags']] for item in ner_dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the different tag values in the dataset:\n",
    "np.unique(np.concatenate(train_labels_ner))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9fd33",
   "metadata": {},
   "source": [
    "### (Optional) Transformer Sequence Tagger\n",
    "\n",
    "People that want to use a transformer for task 2 may want to take a look at the [Token Classification tutorial](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb#scrollTo=vc0BSBLIIrJQ) from HuggingFace. There is no requirement to use a transformer to achieve high marks, this is one option that you may consider. Feel free to skip this part of the notebook if you are using a different kind of model that does not require it.\n",
    "\n",
    "A useful function provided by HuggingFace as part of the Token Classification page is tokenize_and_align. You can reuse this function if you are working with a method that tokenizes the text in a diferent way to the Bio Creative V dataset. This function is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07deb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, max_length=128, is_split_into_words=True)\n",
    "    print(tokenized_inputs.keys())\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# An example of how to use tokenize_and_align:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\") \n",
    "label_all_tokens=False\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b3919",
   "metadata": {},
   "source": [
    "When we get the predictions from the transformer sequence tagger, we will need to skip tokens with a training label of -100, as these are parts of a word or special tokens. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
